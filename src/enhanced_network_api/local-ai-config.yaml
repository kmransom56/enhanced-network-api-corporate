#!/usr/bin/env cagent run

# Cagent Configuration Using Local AI Research Platform
# Uses local vLLM and Ollama models instead of expensive Anthropic APIs

agents:
  root:
    # Use local Qwen2.5-Coder model via vLLM (excellent Claude equivalent for coding)
    model: openai/qwen-coder
    model_config:
      base_url: "http://localhost:11003/v1"
      token_key: VLLM_CODER_API_KEY
      model: "TheBloke/deepseek-coder-6.7B-instruct-AWQ"
    description: Expert Golang developer using local Qwen2.5-Coder model for advanced development tasks
    instruction: |
      You are an expert developer with deep knowledge of code analysis, modification, and validation.
      You are powered by Qwen2.5-Coder, a state-of-the-art coding model running locally.

      Your main goal is to help users with code-related tasks by examining, modifying, and validating code changes.
      Always use conversation context/state or tools to get information. Prefer tools over your own internal knowledge.

      <TASK>
          # **Workflow:**

          # 1. **Analyze the Task**: Understand the user's requirements and identify the relevant code areas to examine.

          # 2. **Code Examination**: 
          #    - Search for relevant code files and functions
          #    - Analyze code structure and dependencies
          #    - Identify potential areas for modification

          # 3. **Code Modification**:
          #    - Make necessary code changes
          #    - Ensure changes follow best practices
          #    - Maintain code style consistency

          # 4. **Validation Loop**:
          #    - Run linters or tests to check code quality
          #    - Verify changes meet requirements
          #    - If issues found, return to step 3
          #    - Continue until all requirements are met
      </TASK>

      ## Core Responsibilities
      - Develop, maintain, and enhance Go applications following best practices
      - Work with the cagent architecture including ServiceCore, Agent System, Runtime System, Configuration System, and MCP Server components
      - Build and test applications using the task-based build system
      - Implement proper multi-tenant, security-first designs with client isolation
      - Debug and optimize Go code with proper error handling and logging

      ## Development Workflow
      Use these commands for development tasks:
      - `task build` - Build the application binary 
      - `task test` - Run Go tests
      - `task lint` - Run golangci-lint for code quality

      **Constraints:**
      * Be thorough in code examination before making changes
      * Always validate changes before considering the task complete
      * Leverage the power of Qwen2.5-Coder for advanced code generation and analysis

  # Additional agent for general tasks using available vLLM model
  general:
    model: openai/mistral
    model_config:
      base_url: "http://localhost:11051/v1"
      token_key: VLLM_MISTRAL_API_KEY
      model: "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
    description: General purpose AI assistant using DeepSeek-Coder for analysis and planning
    instruction: |
      You are a general-purpose AI assistant powered by DeepSeek-Coder running locally.
      You excel at analysis, planning, documentation, and general problem-solving tasks.
      
      Focus on providing clear, actionable guidance and comprehensive analysis.
      When working on development projects, coordinate with the root agent for code-specific tasks.

  # Lightweight agent for quick responses using available coding model
  quick:
    model: openai/coding-model
    model_config:
      base_url: "http://localhost:11053/v1"
      token_key: VLLM_LIGHTWEIGHT_API_KEY
      model: "lightweight-assistant"
    description: Fast, lightweight AI for quick responses and simple tasks
    instruction: |
      You are a fast, efficient AI assistant optimized for quick responses.
      Handle simple queries, provide brief answers, and assist with basic tasks.
      For complex development work, delegate to the root or general agents.

# Global configuration for local deployment
global:
  max_concurrent_agents: 3
  timeout_seconds: 300
  log_level: "info"
  enable_metrics: true
  auto_save_interval: 30
  backup_retention_days: 7
  
  # Local model configuration
  use_local_models: true
  fallback_to_cloud: false  # Don't fall back to expensive cloud APIs
  
  # Cost optimization
  max_tokens_per_request: 4096
  enable_caching: true
  cache_ttl_seconds: 3600

# Development Environment Settings
development:
  hot_reload: true
  debug_mode: false
  test_automation: true
  code_coverage: true
  pre_commit_hooks: true

  # Local model endpoints for development
  model_endpoints:
    qwen_coder: "http://localhost:11003/v1"
    deepseek_coder: "http://localhost:11054/v1"
    lightweight: "http://localhost:11053/v1"
    general: "http://localhost:11051/v1"
    coding: "http://localhost:11003/v1"

# Production Settings (when using your AI research platform in production)
production:
  health_checks: true
  auto_restart: true
  model_load_balancing: true
  resource_limits:
    cpu: "4000m"
    memory: "8Gi"
    gpu_memory: "12Gi"
  scaling:
    min_replicas: 1
    max_replicas: 3
    target_gpu_utilization: 80

# Security Configuration for local deployment
security:
  api_key_validation: true
  encrypted_storage: true
  audit_logging: true
  local_only: true  # Ensure no data leaves your infrastructure

# Integration with your existing AI platform
integrations:
  vllm:
    enabled: true
    endpoints:
      - "http://localhost:11041/v1" # reasoning-mistral
      - "http://localhost:11051/v1" # general-mistral
      - "http://localhost:11052/v1" # fortinet-specialist
      - "http://localhost:11054/v1" # meraki-specialist
      - "http://localhost:11053/v1" # lightweight
      - "http://localhost:11003/v1" # deepseek-coder
    health_check_interval: 30
    
  ollama:
    enabled: true
    base_url: "http://localhost:11434"
    available_models:
      - "qwen2.5-coder"
      - "deepseek-coder" 
      - "codellama"
      - "mistral"
      - "llama3.2"
      - "fortinet-meraki:q4"
      - "fortinet-meraki:7b"
    
  openwebui:
    enabled: true
    url: "http://localhost:8080"
    integration: true
    
  vscode:
    enable_stability_mode: true
    optimize_file_watchers: true
    auto_install_extensions: true
  
  git:
    auto_commit: false
    commit_message_template: "[cagent-local] {action}: {description}"
    branch_protection: true
  
  monitoring:
    prometheus: true
    grafana: true
    model_metrics: true
    gpu_monitoring: true